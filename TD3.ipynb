{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNidCZh1gb7r10djF21INiu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/vault/blob/main/TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "gkNza9le_wuU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNv8MbstpX-N"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Components**"
      ],
      "metadata": {
        "id": "oAls_dh9khDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exemplo da Função do Lidar**"
      ],
      "metadata": {
        "id": "7Z2vgbcr1SoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LidarSensor:\n",
        "    def __init__(self, max_range=10.0):\n",
        "        self.max_range = max_range\n",
        "        self.scan_data = None\n",
        "\n",
        "    def process_scan(self, scan):\n",
        "        scan_range = []\n",
        "        for i in range(len(scan)):\n",
        "            if scan[i] == float('inf'):\n",
        "                scan_range.append(self.max_range)\n",
        "            elif np.isnan(scan[i]):\n",
        "                scan_range.append(0)\n",
        "            else:\n",
        "                scan_range.append(scan[i])\n",
        "\n",
        "        self.scan_data = np.array(scan_range)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Exemplo de dado do lidar\n",
        "    scan = [1.40199554, 1.60912216, 1.61471009, 1.6209532, 1.627864, 1.63545573,\n",
        "            1.64374459, 1.7048229, 1.71486449, 1.72568238, 1.73730004, 1.74974227,\n",
        "            1.76303685, 1.77721334, 1.79230428, 1.80834436, 1.82537198, 1.84342813,\n",
        "            1.86255765, 1.8828088, 1.90423501, 1.92689204, 1.95084333, 1.97615516,\n",
        "            2.00290084, 2.03116107, 2.06102157, 2.0925777, 2.12593293, 2.161201,\n",
        "            2.1985054, 2.23798323, 2.23679996, 2.19318676, 2.15204215, 2.11319757,\n",
        "            2.07650304, 2.04182243, 2.00902891, 1.9780091, 1.97362149, 1.9208858,\n",
        "            1.83375609, 1.86972356, 1.84618092, 1.82390666, 1.80283761, 1.78291857,\n",
        "            1.76409471, 1.74631965, 1.72954786, 1.71373725, 1.69885135, 1.68485391,\n",
        "            1.67171431, 1.65940094, 1.64788854, 1.63714981, 1.62716186, 1.61790514,\n",
        "            1.60935891, 1.60150588, 1.59433031, 1.58781755, 1.58195436, 1.57673013,\n",
        "            1.57213259, 1.56815553, 2.86658835, 4.17488813, 4.68163204, 7.41342402,\n",
        "            7.40879059, 13.50699139, 13.45877552, 13.46566105, 13.52773762, 13.54493904,\n",
        "            13.56733227, 10.67812157, 6.9712891, 7.97131395, 7.26442909, 6.27183151,\n",
        "            5.78773785, 5.297194, 4.90160942, 4.55540562, 4.25638628, 3.99562502,\n",
        "            3.9215436, 3.5702312, 3.52142835, 3.21958733, 3.07313967, 3.02650595,\n",
        "            2.93608308, 2.8213985, 2.71632695, 2.61975694, 2.53074384, 2.44847775,\n",
        "            2.37226176, 2.30149245, 2.23564649, 2.17426372, 2.11694193, 2.0633266,\n",
        "            2.01310396, 1.96599472, 1.9217515, 1.88015223, 1.84099817, 1.804111,\n",
        "            1.76933062, 1.73651075, 1.70552051, 1.67624104, 1.64856386, 1.62239015,\n",
        "            1.64453673, 1.68725729, 1.73293197, 1.78184164, 1.83430707, 1.89069331,\n",
        "            2.04831791, 2.1171186,  2.19155931, 2.27231956, 2.36018777, 1.51011825,\n",
        "            1.4965167, 1.48371518, 1.47758424, 1.55210793, 3.10324192, 3.28008509,\n",
        "            3.47970223, 3.70668197, 3.96694851, 4.26826477, 4.62101316, 5.03939867,\n",
        "            5.54339314, 6.10359621, 1.43871272, 1.40497327, 1.39061165, 1.38594687]\n",
        "\n",
        "    # Criar objeto do sensor LiDAR\n",
        "    lidar = LidarSensor(max_range=10.0)\n",
        "\n",
        "    # Processar os dados do LiDAR\n",
        "    lidar.process_scan(scan)\n",
        "\n",
        "    # Plotar o gráfico das leituras do LiDAR em pontos\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(range(len(lidar.scan_data)), lidar.scan_data, marker='o', color='b')\n",
        "    plt.xlabel('Amostra')\n",
        "    plt.ylabel('Distância (metros)')\n",
        "    plt.title('Leituras do LiDAR')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "A54CKoDGyz8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exemplo da Função do Calculo do Ângulo**"
      ],
      "metadata": {
        "id": "nRyqtHOb1ZSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Posição do robô (exemplo)\n",
        "odom_x, odom_y = 3, 4\n",
        "\n",
        "# Posição do objetivo (exemplo)\n",
        "goal_x, goal_y = 6, 6 # quadrante 1: 6, 6; quadrante 2: 2, 6; quadrante 3: 2, 2; quadrante 4: 6, 2;\n",
        "\n",
        "# Calcular distância até o objetivo\n",
        "distance = np.linalg.norm([odom_x - goal_x, odom_y - goal_y])\n",
        "\n",
        "delta_x = goal_x - odom_x\n",
        "delta_y = goal_y - odom_y\n",
        "\n",
        "# Calcular ângulo entre a orientação do robô e a direção para o objetivo\n",
        "angle = np.arctan2(delta_y, delta_x)\n",
        "theta = angle - np.pi / 4\n",
        "\n",
        "# Criar o gráfico com a seta representando a orientação do robô e a distância até o objetivo\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.arrow(odom_x, odom_y, np.cos(angle), np.sin(angle), head_width=0.2, head_length=0.2, fc='blue', ec='blue')\n",
        "plt.text(odom_x + 0.2, odom_y + 0.2, f'Distância={distance:.2f}, Ângulo={theta:.2f} rad', color='blue')\n",
        "plt.scatter(odom_x, odom_y, color='red', label='Robô')\n",
        "plt.scatter(goal_x, goal_y, color='green', label='Objetivo')\n",
        "plt.xlabel('Eixo x')\n",
        "plt.ylabel('Eixo y')\n",
        "plt.title('Orientação do Robô e Distância até o Objetivo')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlim(0, 10)  # Limites do eixo x de 0 a 10\n",
        "plt.ylim(0, 10)  # Limites do eixo y de 0 a 10\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mQwY4DrF13oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TD3**"
      ],
      "metadata": {
        "id": "h524c52m_oWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.7.4\n",
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "gQNz-4Y-rPTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!apt-get update\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!apt-get install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl"
      ],
      "metadata": {
        "id": "8QENMwxltBaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d box2d-kengz"
      ],
      "metadata": {
        "id": "T2smIt82tbqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Network**"
      ],
      "metadata": {
        "id": "3lZB3r46AWoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KTs6rhe8qnCB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3BCZzBhvr-d",
        "outputId": "8d853b5e-ae82-4ddd-9833-6dc267bf3701"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2349146 sha256=e5bdcd3b74249e5ddb903de0ad1cd51bd2231e1acdc8f1207a98143fda0cdb3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[all]"
      ],
      "metadata": {
        "id": "bt59sLPAv1Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('BipedalWalker-v2')\n",
        "env.seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySwzb6PRqpcL",
        "outputId": "4e2c4531-e497-49c7-a2f3-0a5cac36a3fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: BipedalWalker-v2\n",
            "[2023-08-02 14:59:41,725] Making new env: BipedalWalker-v2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# size of each action\n",
        "action_size = env.action_space\n",
        "print('Size of each action:', action_size.shape[0])\n",
        "\n",
        "# upper bond of each action\n",
        "upper_bond = env.action_space.high\n",
        "print('Upper bond of each action:', upper_bond[0])\n",
        "\n",
        "# lower bond of each action\n",
        "lower_bond = env.action_space.low\n",
        "print('Lower bond of each action:', lower_bond[0])\n",
        "\n",
        "# examine the state space\n",
        "states = env.observation_space\n",
        "state_size = states.shape[0]\n",
        "print('Each observes a state with length: {}'.format(state_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka8XXsteqph5",
        "outputId": "1d02c047-81e1-4345-b1c2-170fda3214ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of each action: 4\n",
            "Upper bond of each action: 1\n",
            "Lower bond of each action: -1\n",
            "Each observes a state with length: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 100        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "YgP_RHskquVM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "6rhWDGNwqwRJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, fc1_units=400, fc2_units=300):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): the maximum valid value for action\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.max_action * torch.tanh(self.fc3(x))\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, fc1_units=400, fc2_units=300):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            fcs1_units (int): Number of nodes in the first hidden layer\n",
        "            fc2_units (int): Number of nodes in the second hidden layer\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size + action_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
        "        state_action = torch.cat([state, action], dim=1)\n",
        "        xs = F.relu(self.fc1(state_action))\n",
        "        x = F.relu(self.fc2(xs))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "m_NtDH5iq1kP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, min_action, random_seed, noise=0.2, noise_std=0.1, noise_clip=0.5):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            random_seed (int): random seed\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.noise = noise\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_clip = noise_clip\n",
        "        self.seed = random.seed(random_seed)\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor_local = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic_local1 = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target1 = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target1.load_state_dict(self.critic_local1.state_dict())\n",
        "        self.critic_optimizer1 = optim.Adam(self.critic_local1.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        self.critic_local2 = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target2 = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target2.load_state_dict(self.critic_local2.state_dict())\n",
        "        self.critic_optimizer2 = optim.Adam(self.critic_local2.parameters(), lr=LR_CRITIC)\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory\"\"\"\n",
        "        # Save experience / reward\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "    def act(self, state, add_noise=True):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "        if add_noise:\n",
        "            # Generate a random noise\n",
        "            noise = np.random.normal(0, self.noise_std, size=self.action_size)\n",
        "            # Add noise to the action for exploration\n",
        "            action = (action + noise).clip(self.min_action[0], self.max_action[0])\n",
        "        self.actor_local.train()\n",
        "        return action\n",
        "\n",
        "    def learn(self, n_iteraion, gamma=GAMMA):\n",
        "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteraion (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            for i in range(n_iteraion):\n",
        "                state, action, reward, next_state, done = self.memory.sample()\n",
        "\n",
        "                action_ = action.cpu().numpy()\n",
        "\n",
        "                # ---------------------------- update critic ---------------------------- #\n",
        "                # Get predicted next-state actions and Q values from target models\n",
        "                actions_next = self.actor_target(next_state)\n",
        "\n",
        "                # Generate a random noise\n",
        "                noise = torch.FloatTensor(action_).data.normal_(0, self.noise).to(device)\n",
        "                noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "                actions_next = (actions_next + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                Q1_targets_next = self.critic_target1(next_state, actions_next)\n",
        "                Q2_targets_next = self.critic_target2(next_state, actions_next)\n",
        "\n",
        "                Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "                # Compute Q targets for current states (y_i)\n",
        "                Q_targets = reward + (gamma * Q_targets_next * (1 - done)).detach()\n",
        "                # Compute critic loss\n",
        "                Q1_expected = self.critic_local1(state, action)\n",
        "                Q2_expected = self.critic_local2(state, action)\n",
        "                critic_loss1 = F.mse_loss(Q1_expected, Q_targets)\n",
        "                critic_loss2 = F.mse_loss(Q2_expected, Q_targets)\n",
        "                # Minimize the loss\n",
        "                self.critic_optimizer1.zero_grad()\n",
        "                critic_loss1.backward()\n",
        "                self.critic_optimizer1.step()\n",
        "\n",
        "                self.critic_optimizer2.zero_grad()\n",
        "                critic_loss2.backward()\n",
        "                self.critic_optimizer2.step()\n",
        "\n",
        "                if i % UPDATE_EVERY_STEP == 0:\n",
        "                    # ---------------------------- update actor ---------------------------- #\n",
        "                    # Compute actor loss\n",
        "                    actions_pred = self.actor_local(state)\n",
        "                    actor_loss = -self.critic_local1(state, actions_pred).mean()\n",
        "                    # Minimize the loss\n",
        "                    self.actor_optimizer.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    self.actor_optimizer.step()\n",
        "\n",
        "                    # ----------------------- update target networks ----------------------- #\n",
        "                    self.soft_update(self.critic_local1, self.critic_target1, TAU)\n",
        "                    self.soft_update(self.critic_local2, self.critic_target2, TAU)\n",
        "                    self.soft_update(self.actor_local, self.actor_target, TAU)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
      ],
      "metadata": {
        "id": "HlmfHeGoq5a1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "                 action_size=env.action_space.shape[0], \\\n",
        "                 max_action=env.action_space.high, \\\n",
        "                 min_action=env.action_space.low, random_seed=0)"
      ],
      "metadata": {
        "id": "pUdfpGl3wFUF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adicionar script ao console desse navegador: `inspecionar` => `console` => `adicionar script`.\n",
        "```\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ],
      "metadata": {
        "id": "3Q3l4acO20iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def td3(n_episodes=1000, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    highest_score = float('-inf')\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done or t==(max_t-1):\n",
        "                agent.learn(t)\n",
        "                break\n",
        "        scores_deque.append(score)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, mean_score, score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if mean_score >= 250 and mean_score > highest_score:\n",
        "            torch.save(agent.actor_local.state_dict(), 'pretrained/bipedal/actor.pth')\n",
        "            torch.save(agent.critic_local1.state_dict(), 'pretrained/bipedal/critic1.pth')\n",
        "            torch.save(agent.critic_local2.state_dict(), 'pretrained/bipedal/critic2.pth')\n",
        "            print('\\rSave at {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            solved = True\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        highest_score = max(highest_score, mean_score)\n",
        "    return scores\n",
        "\n",
        "scores = td3()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(scores)+1), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')"
      ],
      "metadata": {
        "id": "MuInGOo0rIsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "fig = plt.figure(figsize=(16,5))\n",
        "ax = fig.add_subplot(121)\n",
        "plt.plot(np.arange(1, len(scores)+1), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "\n",
        "ay = fig.add_subplot(122)\n",
        "plt.plot(np.arange(1, len(scores)+1), pd.Series(scores).rolling(100).mean())\n",
        "plt.ylabel('Average Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-2boHDV3rKbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.actor_local.load_state_dict(torch.load('pretrained/bipedal/actor.pth'))\n",
        "agent.critic_local1.load_state_dict(torch.load('pretrained/bipedal/critic1.pth'))\n",
        "agent.critic_local2.load_state_dict(torch.load('pretrained/bipedal/critic2.pth'))\n",
        "\n",
        "state = env.reset()\n",
        "while True:\n",
        "    action = agent.act(state)\n",
        "    env.render()\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "1xyGPZ-YrMEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}